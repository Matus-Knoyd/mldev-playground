{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "\n",
    "# Sklearn modules\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid, cross_validate\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, make_scorer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# if git is not installed in docker container\n",
    "os.environ['GIT_PYTHON_REFRESH'] = 'quiet'\n",
    "\n",
    "# silence warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Help functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics related code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cv_metrics(cv_metrics: list[dict]) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Plots cross-validation metrics.\n",
    "\n",
    "    Parameters:\n",
    "        cv_metrics (list[dict]): A list of dictionaries containing cross-validation metrics.\n",
    "        \n",
    "    Returns:\n",
    "        fig (plt.Figure): The generated matplotlib figure.\n",
    "    \"\"\"\n",
    "    # Set plot style to 'fivethirtyeight'\n",
    "    with plt.style.context(style='fivethirtyeight'):\n",
    "        # Calculate the number of rows needed for subplots\n",
    "        rows_needed = int(np.ceil(len(cv_metrics) / 2))\n",
    "        \n",
    "        # Create a subplot figure with the desired dimensions\n",
    "        fig, ax = plt.subplots(rows_needed, 2, figsize=(15, rows_needed * 3))\n",
    "        \n",
    "        # Iterate over each metric in cv_metrics\n",
    "        for index, metric in enumerate(cv_metrics):\n",
    "            # Extract y values for the current metric\n",
    "            y_values = cv_metrics[metric]\n",
    "            \n",
    "            # Generate x values for plotting\n",
    "            x_values = np.arange(len(y_values))\n",
    "    \n",
    "            # Plot the metric on the corresponding subplot\n",
    "            ax[index // 2, index % 2].plot(x_values, y_values) \n",
    "            \n",
    "            # Set title for the subplot\n",
    "            ax[index // 2, index % 2].set_title(metric) \n",
    "    \n",
    "        # Adjust subplot layout for better spacing\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Close the figure to release memory\n",
    "        plt.close(fig)\n",
    "\n",
    "    # Return the generated figure\n",
    "    return fig\n",
    "\n",
    "\n",
    "def compute_metrics(y_true: pd.Series, y_pred: pd.Series, metrics: list, decimals: int = 3, prefix: str = '') -> dict:\n",
    "    \"\"\"\n",
    "    Computes specified metrics between true and predicted values.\n",
    "\n",
    "    Parameters:\n",
    "        y_true (pd.Series): Series of true target values.\n",
    "        y_pred (pd.Series): Series of predicted target values.\n",
    "        metrics (list): List of dictionaries containing metric information. \n",
    "                        Each dictionary should have 'name' and 'function' keys.\n",
    "        decimals (int): Number of decimal places to round the metric values to. Default is 3.\n",
    "        prefix (str): Prefix to add to the metric names in the result dictionary. Default is an empty string.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing computed metrics.\n",
    "    \"\"\"\n",
    "    # Comprehensively compute each metric for y_true and y_pred, rounding to specified decimal places\n",
    "    return {f\"{prefix}{metric['name']}\": round(metric['function'](y_true, y_pred), decimals) for metric in metrics}\n",
    "\n",
    "\n",
    "def compute_aggregated_metrics(cv_metrics: dict, decimals: int = 3) -> dict:\n",
    "    \"\"\"\n",
    "    Computes aggregated statistics for cross-validation metrics.\n",
    "\n",
    "    Parameters:\n",
    "        cv_metrics (dict): Dictionary containing cross-validation metrics.\n",
    "        decimals (int): Number of decimal places to round the aggregated statistics to. Default is 3.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing aggregated statistics (mean, standard deviation, median) for each metric.\n",
    "    \"\"\"\n",
    "    # Initialize an empty dictionary to store aggregated statistics\n",
    "    stats = {}\n",
    "\n",
    "    # Iterate over each metric and its corresponding values\n",
    "    for metric_name, values in cv_metrics.items():\n",
    "        # Compute mean, standard deviation, and median for the values\n",
    "        mean_value = round(np.mean(values), decimals)\n",
    "        std_value = round(np.std(values), decimals)\n",
    "        median_value = round(np.median(values), decimals)\n",
    "        \n",
    "        # Store the computed statistics in the stats dictionary\n",
    "        stats[f'{metric_name}_mean'] = mean_value\n",
    "        stats[f'{metric_name}_std'] = std_value\n",
    "        stats[f'{metric_name}_median'] = median_value\n",
    "    \n",
    "    # Return the dictionary containing aggregated statistics\n",
    "    return stats\n",
    "\n",
    "\n",
    "def make_scorers_dict(metrics: list[dict]) -> dict:\n",
    "    \"\"\"\n",
    "    Creates a dictionary of scorers from a list of metric dictionaries.\n",
    "\n",
    "    Parameters:\n",
    "        metrics (list): List of dictionaries containing metric information. \n",
    "                        Each dictionary should have 'name' and 'scorer' keys.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping metric names to scorer functions.\n",
    "    \"\"\"\n",
    "    # Create a dictionary comprehension to map metric names to scorer functions\n",
    "    return {metric['name']: metric['scorer'] for metric in metrics}\n",
    "\n",
    "\n",
    "def root_mean_squared_error(y_true: pd.Series, y_pred: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the root mean squared error (RMSE) between true and predicted values.\n",
    "\n",
    "    Parameters:\n",
    "        y_true (pd.Series): Series of true target values.\n",
    "        y_pred (pd.Series): Series of predicted target values.\n",
    "\n",
    "    Returns:\n",
    "        float: The root mean squared error between y_true and y_pred.\n",
    "    \"\"\"\n",
    "    # Calculate mean squared error using sklearn's mean_squared_error function\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    \n",
    "    # Return the square root of mean squared error as RMSE\n",
    "    return np.sqrt(mse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search space code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_search_space(search_space: dict[list]) -> list:\n",
    "    \"\"\"\n",
    "    Parses a dictionary representing a search space into a list of combinations.\n",
    "\n",
    "    Parameters:\n",
    "        search_space (dict): A dictionary representing the search space.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of tuples, each containing a combination of objects and parameters.\n",
    "    \"\"\"\n",
    "    parsed_steps = {}\n",
    "\n",
    "    # Iterate over each step in the search space\n",
    "    for step, step_objects in search_space.items():\n",
    "        step_data = []\n",
    "        \n",
    "        # Iterate over each object within the current step\n",
    "        for step_object in step_objects:\n",
    "            obj = step_object.get('object')\n",
    "            params = step_object.get('params')\n",
    "            \n",
    "            # If object exists\n",
    "            if obj:\n",
    "                if params:\n",
    "                    # Generate combinations of object and parameters using ParameterGrid\n",
    "                    step_data += [obj(**p) for p in ParameterGrid(params)]\n",
    "                else:\n",
    "                    # If no parameters, simply add the object\n",
    "                    step_data.append(obj())\n",
    "            else:\n",
    "                # If object is None, append None\n",
    "                step_data.append(obj)\n",
    "        \n",
    "        # Store parsed data for the current step\n",
    "        parsed_steps[step] = step_data\n",
    "\n",
    "    # Generate combinations of parsed steps\n",
    "    return [\n",
    "        tuple(zip(parsed_steps.keys(), combination)) \n",
    "        for combination in product(*parsed_steps.values())\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "The `search_space` dictionary represents a space of possible configurations for a machine learning pipeline, typically used in hyperparameter tuning or model selection processes. It consists of two main components: scalers and models.\n",
    "\n",
    "#### 1. Scalers:\n",
    "- The `'scaler'` key contains a list of dictionaries, each representing a scaler to be used in the pipeline.\n",
    "- Each dictionary contains an `'object'` key, which refers to the scaler class to be used. If `'object'` is `None`, it indicates no scaling will be applied.\n",
    "- Example scalers included are `StandardScaler` and `MinMaxScaler`.\n",
    "\n",
    "#### 2. Models:\n",
    "- The `'model'` key contains a list of dictionaries, each representing a machine learning model along with its hyperparameters.\n",
    "- Each dictionary contains an `'object'` key, referring to the model class to be used, and a `'params'` key, which holds a dictionary of hyperparameters and their corresponding values to be explored.\n",
    "- Example models included are `RandomForestRegressor` and `ElasticNet`.\n",
    "- For `RandomForestRegressor`, hyperparameters such as `'n_estimators'` and `'max_depth'` are specified with a range of values to be explored.\n",
    "- For `ElasticNet`, hyperparameters such as `'alpha'` and `'l1_ratio'` are specified with arrays of values to be explored.\n",
    "\n",
    "Overall, `search_space` encapsulates a range of possible configurations for a machine learning pipeline, including different scalers and models with various hyperparameter settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    'scaler': [\n",
    "        {\n",
    "            'object': None\n",
    "        },\n",
    "        {\n",
    "            'object': StandardScaler\n",
    "        },\n",
    "        {\n",
    "            'object': MinMaxScaler\n",
    "        }\n",
    "    ],\n",
    "    'model': [\n",
    "        {\n",
    "            'object': RandomForestRegressor,\n",
    "            'params': {\n",
    "                'n_estimators': [10, 100, 1000],\n",
    "                'max_depth': [10, None]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'object': ElasticNet,\n",
    "            'params': {\n",
    "                'alpha': np.arange(0, 1, 0.5),\n",
    "                'l1_ratio': np.arange(0, 1, 0.5)\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "The `metrics` list contains dictionaries, each representing a metric used to evaluate model performance. Each dictionary consists of the following keys:\n",
    "\n",
    "Each metric has an associated function to compute it and a scorer created using `make_scorer`. The `greater_is_better` parameter specifies whether higher values of the metric indicate better performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    {\n",
    "        'name': 'mean_absolute_error',\n",
    "        'function': mean_absolute_error,\n",
    "        'scorer': make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "    },\n",
    "    {\n",
    "        'name': 'mean_squared_error',\n",
    "        'function': mean_squared_error,\n",
    "        'scorer': make_scorer(mean_squared_error, greater_is_better=False)\n",
    "    },\n",
    "    {\n",
    "        'name': 'root_mean_squared_error',\n",
    "        'function': root_mean_squared_error,\n",
    "        'scorer': make_scorer(root_mean_squared_error, greater_is_better=False)\n",
    "    },\n",
    "    {\n",
    "        'name': 'r2_score',\n",
    "        'function': r2_score,\n",
    "        'scorer': make_scorer(r2_score, greater_is_better=True)\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "These configurations specify settings for tracking experiments, naming the experiment, defining the model name, and specifying the path to store the model artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACKING_URI = \"http://tracking_server:5000\"\n",
    "EXPERIMENT_NAME = \"regression-diabetes\"\n",
    "MODEL_NAME = \"diabetes_model\"\n",
    "MODEL_ARTIFACT_PATH = 'model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the tracking URI\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "\n",
    "# Set the experiment name\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file path of the dataset\n",
    "filename = './data/diabetes.csv'\n",
    "\n",
    "# Define the name of the target variable in the dataset\n",
    "target_variable = 'target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the file\n",
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and holdout sets\n",
    "df_train, df_holdout = train_test_split(\n",
    "    df, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Extract features (X) and target variable (y) from the training set\n",
    "X_train = df_train.drop(target_variable, axis=1)  \n",
    "y_train = df_train[target_variable]              \n",
    "\n",
    "# Extract features (X) and target variable (y) from the holdout set\n",
    "X_holdout = df_holdout.drop(target_variable, axis=1)  \n",
    "y_holdout = df_holdout[target_variable]              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MLflow datasets from Pandas DataFrames for training and holdout sets\n",
    "train_dataset = mlflow.data.from_pandas(df_train, source=filename, targets=target_variable)\n",
    "holdout_dataset = mlflow.data.from_pandas(df_holdout, source=filename, targets=target_variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the limit for the number of pipeline configurations to explore\n",
    "LIMIT = 2\n",
    "\n",
    "# Iterate over the first LIMIT pipeline configurations from the search space\n",
    "for pipeline_steps in parse_search_space(search_space)[:LIMIT]:\n",
    "    # Create a pipeline using the current configuration\n",
    "    pipeline = Pipeline(pipeline_steps)\n",
    "    \n",
    "    # Define tags for the MLflow run\n",
    "    tags = {\n",
    "        'estimator_name': type(pipeline['model']).__name__,  # Name of the estimator\n",
    "        'estimator_class': str(type(pipeline['model']))      # Class of the estimator\n",
    "    }\n",
    "    \n",
    "    # Start an MLflow run with the defined tags\n",
    "    with mlflow.start_run(tags=tags) as run:\n",
    "        # Fit the pipeline on the training data\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # Log the pipeline as a MLflow model artifact\n",
    "        mlflow.sklearn.log_model(\n",
    "            sk_model=pipeline, \n",
    "            artifact_path=MODEL_ARTIFACT_PATH, \n",
    "            signature=mlflow.models.infer_signature(\n",
    "                model_input=X_train, \n",
    "                model_output=pipeline.predict(X_train)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Log pipeline parameters\n",
    "        pipeline_params = pipeline.get_params()\n",
    "        mlflow.log_params(pipeline_params)\n",
    "\n",
    "        # Evaluate pipeline using cross-validation on the training data\n",
    "        cv_metrics = cross_validate(\n",
    "            estimator=pipeline, \n",
    "            X=X_train, \n",
    "            y=y_train, \n",
    "            cv=5,\n",
    "            return_train_score=True, \n",
    "            scoring=make_scorers_dict(metrics)\n",
    "        )\n",
    "        cv_metrics_aggregated = compute_aggregated_metrics(cv_metrics)\n",
    "        mlflow.log_metrics(cv_metrics_aggregated)\n",
    "\n",
    "        # Plot cross-validation metrics and log the figure\n",
    "        cv_fig = plot_cv_metrics(cv_metrics)\n",
    "        mlflow.log_figure(cv_fig, \"graphs/cross_validation_metrics.png\")\n",
    "\n",
    "        # Evaluate pipeline on holdout data\n",
    "        holdout_metrics = compute_metrics(\n",
    "            y_true=y_holdout, \n",
    "            y_pred=pipeline.predict(X_holdout), \n",
    "            metrics=metrics, \n",
    "            prefix='holdout_'\n",
    "        )\n",
    "        mlflow.log_metrics(holdout_metrics)\n",
    "\n",
    "        # Log SHAP explanations for the holdout predictions\n",
    "        mlflow.shap.log_explanation(pipeline.predict, X_holdout)\n",
    "\n",
    "        # Log datasets used for training and holdout\n",
    "        mlflow.log_input(train_dataset, context=\"training\")\n",
    "        mlflow.log_input(holdout_dataset, context=\"holdout\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow import MlflowClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The client will allow interaction with MLflow tracking server to query runs, metrics, parameters, artifacts, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an MLflow client\n",
    "client = MlflowClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create registered model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to create a registered model in MLflow, throws the error if model already exists\n",
    "try:\n",
    "    client.create_registered_model(\n",
    "        name=MODEL_NAME,                                \n",
    "        tags={'experiment': EXPERIMENT_NAME},           \n",
    "        description='Model for diabetes prediction'     \n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the best model from the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the experiment from MLflow by its name\n",
    "experiment = client.get_experiment_by_name(name=EXPERIMENT_NAME)\n",
    "experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for runs within the specified experiment using MLflow's search_runs function\n",
    "df_runs = mlflow.search_runs(experiment_ids=experiment.experiment_id)\n",
    "df_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame of runs by the 'holdout_mean_absolute_error' metric in ascending order,\n",
    "# select the top row (i.e., the run with the lowest holdout MAE),\n",
    "# reset the index to start from 0, and drop the original index\n",
    "df_best_run = df_runs.sort_values(by='metrics.holdout_mean_absolute_error').head(1).reset_index(drop=True)\n",
    "df_best_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the run ID of the best run from the DataFrame\n",
    "run_id = df_best_run.loc[0, 'run_id']\n",
    "\n",
    "# Get the artifact URI of the best run from the DataFrame\n",
    "artifact_uri = df_best_run.loc[0, 'artifact_uri']\n",
    "\n",
    "# Construct the source path for the model artifact using the artifact URI and MODEL_ARTIFACT_PATH\n",
    "model_source = f\"{artifact_uri}/{MODEL_ARTIFACT_PATH}\"\n",
    "\n",
    "print(\"Run ID:\", run_id)\n",
    "print(\"Artifact URI:\", artifact_uri)\n",
    "print(\"Model Source:\", model_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model version in MLflow\n",
    "mv = client.create_model_version(\n",
    "    name=MODEL_NAME,        # Name of the registered model\n",
    "    source=model_source,    # Source path of the model artifact\n",
    "    run_id=run_id           # ID of the MLflow run associated with the model version\n",
    ")\n",
    "mv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set alias name \n",
    "ALIAS = \"staging\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set an alias for the specified model version in MLflow\n",
    "client.set_registered_model_alias(\n",
    "    name=MODEL_NAME,          # Name of the registered model\n",
    "    alias=ALIAS,          # Alias name for the model version\n",
    "    version=mv.version        # Version number of the model version\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load registered model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with the specified alias from MLflow\n",
    "model_staging = mlflow.pyfunc.load_model(f\"models:/{MODEL_NAME}@staging\")\n",
    "model_staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the target variable for the holdout dataset using the staged model loaded from MLflow\n",
    "y_pred_holdout = model_staging.predict(X_holdout)\n",
    "y_pred_holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve metadata information about the staged model loaded from MLflow\n",
    "model_metadata = model_staging.metadata.to_dict()\n",
    "model_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
