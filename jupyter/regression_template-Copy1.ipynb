{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "\n",
    "# Sklearn modules\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid, cross_validate\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, make_scorer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# if git is not installed in docker container\n",
    "os.environ['GIT_PYTHON_REFRESH'] = 'quiet'\n",
    "\n",
    "# silence warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics related code\n",
    "\n",
    "def plot_cv_metrics(cv_metrics: list[dict]):\n",
    "    with plt.style.context(style='fivethirtyeight'):\n",
    "        rows_needed = int(np.ceil(len(cv_metrics)/2))\n",
    "        \n",
    "        fig, ax = plt.subplots(rows_needed, 2, figsize=(15, rows_needed*3)) \n",
    "        for index, metric in enumerate(cv_metrics):\n",
    "            y_values = cv_metrics[metric]\n",
    "            x_values = np.arange(len(y_values))\n",
    "    \n",
    "            ax[index//2, index%2].plot(x_values, y_values) \n",
    "            ax[index//2, index%2].set_title(metric) \n",
    "    \n",
    "        plt.tight_layout()\n",
    "        plt.close(fig)\n",
    "\n",
    "    return fig\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_cv_metrics(cv_metrics: list[dict]) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Plots cross-validation metrics.\n",
    "\n",
    "    Parameters:\n",
    "        cv_metrics (list[dict]): A list of dictionaries containing cross-validation metrics.\n",
    "        \n",
    "    Returns:\n",
    "        fig (plt.Figure): The generated matplotlib figure.\n",
    "    \"\"\"\n",
    "    # Set plot style to 'fivethirtyeight'\n",
    "    with plt.style.context(style='fivethirtyeight'):\n",
    "        # Calculate the number of rows needed for subplots\n",
    "        rows_needed = int(np.ceil(len(cv_metrics) / 2))\n",
    "        \n",
    "        # Create a subplot figure with the desired dimensions\n",
    "        fig, ax = plt.subplots(rows_needed, 2, figsize=(15, rows_needed * 3))\n",
    "        \n",
    "        # Iterate over each metric in cv_metrics\n",
    "        for index, metric in enumerate(cv_metrics):\n",
    "            # Extract y values for the current metric\n",
    "            y_values = cv_metrics[metric]\n",
    "            \n",
    "            # Generate x values for plotting\n",
    "            x_values = np.arange(len(y_values))\n",
    "    \n",
    "            # Plot the metric on the corresponding subplot\n",
    "            ax[index // 2, index % 2].plot(x_values, y_values) \n",
    "            \n",
    "            # Set title for the subplot\n",
    "            ax[index // 2, index % 2].set_title(metric) \n",
    "    \n",
    "        # Adjust subplot layout for better spacing\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Close the figure to release memory\n",
    "        plt.close(fig)\n",
    "\n",
    "    # Return the generated figure\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_search_space(search_space: dict) -> list:\n",
    "    parsed_steps = {}\n",
    "    for step, step_objects in search_space.items():\n",
    "        step_data = []\n",
    "        for step_object in step_objects:\n",
    "            obj = step_object.get('object')\n",
    "            params = step_object.get('params')\n",
    "            if obj:\n",
    "                if params:\n",
    "                    step_data += [obj(**p) for p in ParameterGrid(params)]\n",
    "                else:\n",
    "                    step_data.append(obj())\n",
    "            else:\n",
    "                step_data.append(obj)\n",
    "        parsed_steps[step] = step_data\n",
    "\n",
    "    return [\n",
    "        tuple(zip(parsed_steps.keys(), combination)) \n",
    "        for combination in product(*parsed_steps.values())\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true: pd.Series, y_pred: pd.Series) -> float:\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true: pd.Series, y_pred: pd.Series, metrics: list, decimals: int = 3, prefix: str = '') -> dict:\n",
    "    return {f\"{prefix}{metric['name']}\": round(metric['function'](y_true, y_pred), decimals) for metric in metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_aggregated_metrics(cv_metrics: dict, decimals: int = 3) -> dict:\n",
    "    stats = {}\n",
    "    for metric_name, values in cv_metrics.items():\n",
    "        stats[f'{metric_name}_mean'] = round(np.mean(values), decimals)\n",
    "        stats[f'{metric_name}_std'] = round(np.std(values), decimals)\n",
    "        stats[f'{metric_name}_median'] = round(np.median(values), decimals)\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_scorers_dict(metrics: list) -> dict:\n",
    "    return {metric['name']: metric['scorer'] for metric in metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    'scaler': [\n",
    "        {\n",
    "            'object': None\n",
    "        },\n",
    "        {\n",
    "            'object': StandardScaler\n",
    "        },\n",
    "        {\n",
    "            'object': MinMaxScaler\n",
    "        }\n",
    "    ],\n",
    "    'model': [\n",
    "        {\n",
    "            'object': RandomForestRegressor,\n",
    "            'params': {\n",
    "                'n_estimators': [10, 100, 1000],\n",
    "                'max_depth': [10, None]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'object': ElasticNet,\n",
    "            'params': {\n",
    "                'alpha': np.arange(0, 1, 0.2),\n",
    "                'l1_ratio': np.arange(0, 1, 0.2)\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    {\n",
    "        'name': 'mean_absolute_error',\n",
    "        'function': mean_absolute_error,\n",
    "        'scorer': make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "    },\n",
    "    {\n",
    "        'name': 'mean_squared_error',\n",
    "        'function': mean_squared_error,\n",
    "        'scorer': make_scorer(mean_squared_error, greater_is_better=False)\n",
    "    },\n",
    "    {\n",
    "        'name': 'root_mean_squared_error',\n",
    "        'function': root_mean_squared_error,\n",
    "        'scorer': make_scorer(root_mean_squared_error, greater_is_better=False)\n",
    "    },\n",
    "    {\n",
    "        'name': 'r2_score',\n",
    "        'function': r2_score,\n",
    "        'scorer': make_scorer(r2_score, greater_is_better=True)\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACKING_URI = \"http://tracking_server:5000\"\n",
    "EXPERIMENT_NAME = \"diabetes\"\n",
    "MODEL_NAME = \"diabetes_model\"\n",
    "MODEL_ARTIFACT_PATH = 'model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './data/diabetes.csv'\n",
    "target_variable = 'target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_holdout = train_test_split(\n",
    "    df, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "X_train = df_train.drop(target_variable, axis=1)\n",
    "y_train = df_train[target_variable]\n",
    "\n",
    "X_holdout = df_holdout.drop(target_variable, axis=1)\n",
    "y_holdout = df_holdout[target_variable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = mlflow.data.from_pandas(df_train, source=filename, targets=target_variable)\n",
    "holdout_dataset = mlflow.data.from_pandas(df_holdout, source=filename, targets=target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pipeline_steps in parse_search_space(search_space)[:2]:\n",
    "    pipeline = Pipeline(pipeline_steps)\n",
    "    \n",
    "    tags = {\n",
    "        'estimator_name': type(pipeline['model']).__name__,\n",
    "        'estimator_class': str(type(pipeline['model']))\n",
    "    }\n",
    "    \n",
    "    with mlflow.start_run(tags=tags) as run:\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # log pipeline\n",
    "        mlflow.sklearn.log_model(\n",
    "            sk_model = pipeline, \n",
    "            artifact_path = MODEL_ARTIFACT_PATH, \n",
    "            signature = mlflow.models.infer_signature(\n",
    "                model_input = X_train, \n",
    "                model_output = pipeline.predict(X_train)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # pipeline params\n",
    "        pipeline_params = pipeline.get_params()\n",
    "        mlflow.log_params(pipeline_params)\n",
    "\n",
    "        # metrics train + test \n",
    "        cv_metrics = cross_validate(\n",
    "            estimator = pipeline, \n",
    "            X = X_train, \n",
    "            y = y_train, \n",
    "            cv = 5,\n",
    "            return_train_score = True, \n",
    "            scoring = make_scorers_dict(metrics)\n",
    "        )\n",
    "        cv_metrics_aggregated = compute_aggregated_metrics(cv_metrics)\n",
    "        mlflow.log_metrics(cv_metrics_aggregated)\n",
    "\n",
    "        # cv metrics plot\n",
    "        cv_fig = plot_cv_metrics(cv_metrics)\n",
    "        mlflow.log_figure(cv_fig, \"graphs/cross_validation_metrics.png\")\n",
    "\n",
    "        # metrics holdout\n",
    "        holdout_metrics = compute_metrics(\n",
    "            y_true = y_holdout, \n",
    "            y_pred = pipeline.predict(X_holdout), \n",
    "            metrics = metrics, \n",
    "            prefix = 'holdout_'\n",
    "        )\n",
    "        mlflow.log_metrics(holdout_metrics)\n",
    "\n",
    "        # shap values\n",
    "        mlflow.shap.log_explanation(pipeline.predict, X_holdout)\n",
    "\n",
    "        # log dataset\n",
    "        mlflow.log_input(train_dataset, context=\"training\")\n",
    "        mlflow.log_input(holdout_dataset, context=\"holdout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow import MlflowClient\n",
    "\n",
    "client = MlflowClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    client.create_registered_model(\n",
    "        name = MODEL_NAME,\n",
    "        tags = {\n",
    "            'expriment': EXPERIMENT_NAME,\n",
    "        },\n",
    "        description = 'Model for diabetes prediction'\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = client.get_experiment_by_name(name=EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_runs = mlflow.search_runs(experiment_ids=experiment.experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_run = df_runs.sort_values(by='metrics.holdout_mean_absolute_error').head(1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = df_best_run.loc[0, 'run_id']\n",
    "artifact_uri = df_best_run['artifact_uri'][0]\n",
    "model_source = f\"{artifact_uri}/{MODEL_ARTIFACT_PATH}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv = client.create_model_version(\n",
    "    name = MODEL_NAME, \n",
    "    source = model_source, \n",
    "    run_id = run_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.set_registered_model_alias(\n",
    "    name = MODEL_NAME, \n",
    "    alias = \"staging\", \n",
    "    version = mv.version\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_staging = mlflow.pyfunc.load_model(f\"models:/{MODEL_NAME}@staging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_staging.predict(X_holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_staging.metadata.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
